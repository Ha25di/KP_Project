{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 2 Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're missing any packages, uncomment and run:\n",
    "# !pip install torch torchvision torchaudio transformers datasets evaluate --upgrade\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subset size: 2000\n",
      "Test subset size: 200\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=128):\n",
    "    # The AG News dataset has a \"text\" field. We'll tokenize that.\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "# 1. Load dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# 2. Shuffle if you want a random subset\n",
    "shuffled_train = dataset[\"train\"].shuffle(seed=42)\n",
    "shuffled_test = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# 3. Select smaller subsets for demonstration (adjust sizes as needed)\n",
    "subset_size = 2000  # e.g., 2k train samples\n",
    "train_subset = shuffled_train.select(range(subset_size))\n",
    "\n",
    "test_subset_size = 200  # e.g., 200 test samples\n",
    "test_subset = shuffled_test.select(range(test_subset_size))\n",
    "\n",
    "# 4. Load GPT-2 tokenizer & model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# GPT-2 has no pad token; use EOS as pad\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 5. Tokenize subsets\n",
    "def tokenize_function(examples):\n",
    "    return preprocess_function(examples, tokenizer)\n",
    "\n",
    "tokenized_train_subset = train_subset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"label\"]  # remove label column\n",
    ")\n",
    "\n",
    "tokenized_test_subset = test_subset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"label\"]\n",
    ")\n",
    "\n",
    "len_train = len(tokenized_train_subset)\n",
    "len_test = len(tokenized_test_subset)\n",
    "\n",
    "print(f\"Training subset size: {len_train}\")\n",
    "print(f\"Test subset size: {len_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadi/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # GPT-2 uses causal LM, not masked LM\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-agnews-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,              # Increase if you want more training\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    # fp16=torch.cuda.is_available(),  # Use FP16 if GPU is available\n",
    "    fp16=False,  \n",
    "    push_to_hub=False,\n",
    "    no_cuda=True,  # ensures we stay on CPU and never use GPU / MPS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5505b963d94f1fa035896822823158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4571, 'grad_norm': 20.659204483032227, 'learning_rate': 2.5e-05, 'epoch': 0.05}\n",
      "{'loss': 4.1328, 'grad_norm': 19.32101821899414, 'learning_rate': 5e-05, 'epoch': 0.1}\n",
      "{'loss': 4.0734, 'grad_norm': 20.103322982788086, 'learning_rate': 4.868421052631579e-05, 'epoch': 0.15}\n",
      "{'loss': 3.8476, 'grad_norm': 18.872285842895508, 'learning_rate': 4.736842105263158e-05, 'epoch': 0.2}\n",
      "{'loss': 3.937, 'grad_norm': 18.266101837158203, 'learning_rate': 4.605263157894737e-05, 'epoch': 0.25}\n",
      "{'loss': 3.9301, 'grad_norm': 16.458646774291992, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.3}\n",
      "{'loss': 3.8701, 'grad_norm': 13.638636589050293, 'learning_rate': 4.342105263157895e-05, 'epoch': 0.35}\n",
      "{'loss': 3.8596, 'grad_norm': 16.832426071166992, 'learning_rate': 4.210526315789474e-05, 'epoch': 0.4}\n",
      "{'loss': 3.8464, 'grad_norm': 11.293535232543945, 'learning_rate': 4.078947368421053e-05, 'epoch': 0.45}\n",
      "{'loss': 3.7103, 'grad_norm': 13.561971664428711, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.5}\n",
      "{'loss': 3.8088, 'grad_norm': 12.614561080932617, 'learning_rate': 3.815789473684211e-05, 'epoch': 0.55}\n",
      "{'loss': 3.718, 'grad_norm': 11.565779685974121, 'learning_rate': 3.6842105263157895e-05, 'epoch': 0.6}\n",
      "{'loss': 3.7642, 'grad_norm': 7.656428337097168, 'learning_rate': 3.5526315789473684e-05, 'epoch': 0.65}\n",
      "{'loss': 3.657, 'grad_norm': 9.282767295837402, 'learning_rate': 3.421052631578947e-05, 'epoch': 0.7}\n",
      "{'loss': 3.7952, 'grad_norm': 8.828340530395508, 'learning_rate': 3.289473684210527e-05, 'epoch': 0.75}\n",
      "{'loss': 3.7695, 'grad_norm': 7.833470344543457, 'learning_rate': 3.157894736842105e-05, 'epoch': 0.8}\n",
      "{'loss': 3.6423, 'grad_norm': 10.822136878967285, 'learning_rate': 3.0263157894736844e-05, 'epoch': 0.85}\n",
      "{'loss': 3.7716, 'grad_norm': 7.569299221038818, 'learning_rate': 2.8947368421052634e-05, 'epoch': 0.9}\n",
      "{'loss': 3.7607, 'grad_norm': 9.014138221740723, 'learning_rate': 2.7631578947368426e-05, 'epoch': 0.95}\n",
      "{'loss': 3.5923, 'grad_norm': 8.99286937713623, 'learning_rate': 2.6315789473684212e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3972c5278af043dd958cafc2a8a9a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.540146589279175, 'eval_runtime': 11.3574, 'eval_samples_per_second': 17.61, 'eval_steps_per_second': 8.805, 'epoch': 1.0}\n",
      "{'loss': 3.0896, 'grad_norm': 8.752106666564941, 'learning_rate': 2.5e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1378, 'grad_norm': 7.898033142089844, 'learning_rate': 2.368421052631579e-05, 'epoch': 1.1}\n",
      "{'loss': 3.141, 'grad_norm': 12.243232727050781, 'learning_rate': 2.236842105263158e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0856, 'grad_norm': 17.736480712890625, 'learning_rate': 2.105263157894737e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1723, 'grad_norm': 9.557938575744629, 'learning_rate': 1.9736842105263158e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1611, 'grad_norm': 9.12528133392334, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.3}\n",
      "{'loss': 3.0941, 'grad_norm': 8.969873428344727, 'learning_rate': 1.7105263157894737e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1977, 'grad_norm': 9.17868423461914, 'learning_rate': 1.5789473684210526e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2109, 'grad_norm': 7.272565841674805, 'learning_rate': 1.4473684210526317e-05, 'epoch': 1.45}\n",
      "{'loss': 2.9573, 'grad_norm': 9.294599533081055, 'learning_rate': 1.3157894736842106e-05, 'epoch': 1.5}\n",
      "{'loss': 3.159, 'grad_norm': 10.058106422424316, 'learning_rate': 1.1842105263157895e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0962, 'grad_norm': 11.67076587677002, 'learning_rate': 1.0526315789473684e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1206, 'grad_norm': 9.542130470275879, 'learning_rate': 9.210526315789474e-06, 'epoch': 1.65}\n",
      "{'loss': 3.0394, 'grad_norm': 10.570011138916016, 'learning_rate': 7.894736842105263e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2109, 'grad_norm': 10.651594161987305, 'learning_rate': 6.578947368421053e-06, 'epoch': 1.75}\n",
      "{'loss': 3.2059, 'grad_norm': 6.127892971038818, 'learning_rate': 5.263157894736842e-06, 'epoch': 1.8}\n",
      "{'loss': 3.0276, 'grad_norm': 9.497751235961914, 'learning_rate': 3.9473684210526315e-06, 'epoch': 1.85}\n",
      "{'loss': 3.1247, 'grad_norm': 10.561150550842285, 'learning_rate': 2.631578947368421e-06, 'epoch': 1.9}\n",
      "{'loss': 3.0343, 'grad_norm': 10.34608268737793, 'learning_rate': 1.3157894736842106e-06, 'epoch': 1.95}\n",
      "{'loss': 3.0106, 'grad_norm': 11.224472045898438, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b49c1a21664c4997eb683586ce415d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5496771335601807, 'eval_runtime': 11.4977, 'eval_samples_per_second': 17.395, 'eval_steps_per_second': 8.697, 'epoch': 2.0}\n",
      "{'train_runtime': 1247.937, 'train_samples_per_second': 3.205, 'train_steps_per_second': 1.603, 'train_loss': 3.4805075759887694, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-agnews-finetuned/tokenizer_config.json',\n",
       " './gpt2-agnews-finetuned/special_tokens_map.json',\n",
       " './gpt2-agnews-finetuned/vocab.json',\n",
       " './gpt2-agnews-finetuned/merges.txt',\n",
       " './gpt2-agnews-finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_subset,\n",
    "    eval_dataset=tokenized_test_subset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model & tokenizer\n",
    "trainer.save_model(\"./gpt2-agnews-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-agnews-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Texts:\n",
      "1. Breaking news from the tech world: Google is now #39;s #39; second largest advertising vendor, and it's now #39;s #39; #39; #39; #39; #39; #39; #39; #39; #39; with nearly 30\n",
      "\n",
      "2. Breaking news from the tech world: Apple #39;s new  quot;PC will be much better for business  quot; customers, Apple CEO Tim Cook told shareholders Wednesday. quot;Apple #39;s new  quot;PC will be much better for business quot; customers, Apple CEO\n",
      "\n",
      "3. Breaking news from the tech world: #39;s biggest companies look to digital to help bring profit down after rising costs were blamed for some of the largest losses on the tech industry last year. Read more here.  quot;\\$151 million in net profit yesterday. quot; #39;\n",
      "\n",
      "4. Breaking news from the tech world: The UK Government has launched a public survey to better understand the impact of digital technology on the economy.   The results of that public survey were posted yesterday and will be used to guide the decision-making process  as well as the wider enterprise sector.  \n",
      "\n",
      "5. Breaking news from the tech world: Apple has posted a surge in quarterly profit, and it continues to sell more gadgets than ever before. Google has posted its first quarterly loss for three years, and Google shares rose on news reports that Apple will buy its own mobile phone business. The Dow Jones Industrial Average\n",
      "\n",
      "6. Breaking news from the tech world: More details of the Linux Kernel 2.0 security patch can be found here. It is important to keep in mind that many of the features of the kernel 2.0 series, including bug fixes, security patches, and more, have not yet been released. That\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload the fine-tuned model\n",
    "fine_tuned_model_path = \"./gpt2-agnews-finetuned\"\n",
    "finetuned_tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, num_return_sequences=1):\n",
    "    model.eval()\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            top_k=50\n",
    "        )\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Breaking news from the tech world:\"\n",
    "gen_texts = generate_text(\n",
    "    finetuned_model, \n",
    "    finetuned_tokenizer, \n",
    "    prompt,\n",
    "    max_length=60,\n",
    "    num_return_sequences=6\n",
    ")\n",
    "\n",
    "print(\"Generated Texts:\")\n",
    "for idx, t in enumerate(gen_texts, start=1):\n",
    "    print(f\"{idx}. {t}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity: 7.46\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts, device, max_length=512):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a list of texts using the provided model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model: The language model (e.g., GPT2LMHeadModel).\n",
    "        tokenizer: The tokenizer corresponding to the model.\n",
    "        texts (list of str): The generated texts to evaluate.\n",
    "        device: The computation device (CPU or GPU).\n",
    "        max_length (int): The maximum token length for the inputs.\n",
    "\n",
    "    Returns:\n",
    "        float: The average perplexity across all texts.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Tokenize input text\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "\n",
    "            # The labels are the same as inputs for language modeling\n",
    "            labels = inputs[\"input_ids\"]\n",
    "\n",
    "            # Forward pass to compute loss\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n += 1\n",
    "\n",
    "    avg_loss = total_loss / n\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# Compute Perplexity\n",
    "perplexity = compute_perplexity(\n",
    "    model=finetuned_model,\n",
    "    tokenizer=finetuned_tokenizer,\n",
    "    texts=gen_texts,\n",
    "    device=device  # 'cpu' in your case\n",
    ")\n",
    "\n",
    "print(f\"Average Perplexity: {perplexity:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-BLEU Score: 0.1970\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_self_bleu(samples):\n",
    "    \"\"\"\n",
    "    Compute Self-BLEU score for a list of generated samples.\n",
    "    \"\"\"\n",
    "    if len(samples) < 2:\n",
    "        return 0.0  # Undefined for single sample\n",
    "\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    scores = []\n",
    "\n",
    "    for i, pred in enumerate(samples):\n",
    "        # Prediction is the current sample as a string\n",
    "        prediction = pred.strip()\n",
    "\n",
    "        # References are all other samples as a list of strings\n",
    "        references = [other_pred.strip() for j, other_pred in enumerate(samples) if j != i]\n",
    "\n",
    "        # Compute BLEU for this prediction against all other references\n",
    "        result = bleu.compute(\n",
    "            predictions=[prediction],\n",
    "            references=[references]  # Note: references should be a list of lists\n",
    "        )\n",
    "\n",
    "        scores.append(result[\"bleu\"])\n",
    "\n",
    "    # Average Self-BLEU score\n",
    "    average_self_bleu = sum(scores) / len(scores)\n",
    "    return average_self_bleu\n",
    "\n",
    "# Assuming 'gen_texts' is your list of generated strings\n",
    "self_bleu = compute_self_bleu(gen_texts)\n",
    "print(f\"Self-BLEU Score: {self_bleu:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "def calculate_coherence(texts):\n",
    "    \"\"\"\n",
    "    Calculate coherence of generated texts based on cosine similarity of sentence embeddings.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of generated texts.\n",
    "\n",
    "    Returns:\n",
    "        list: Coherence scores for each text.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Pre-trained sentence embedding model\n",
    "    scores = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Split text into sentences\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        if len(sentences) < 2:  # At least 2 sentences needed for coherence\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Compute sentence embeddings\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute pairwise cosine similarities between consecutive sentences\n",
    "        similarities = cosine_similarity(embeddings[:-1].cpu(), embeddings[1:].cpu())\n",
    "        avg_similarity = np.mean(similarities)  # Average cosine similarity between consecutive sentences\n",
    "        scores.append(avg_similarity)\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Breaking news from the tech world: Google is now #39;s #39; second largest advertising vendor, and it's now #39;s #39; #39; #39; #39; #39; #39; #39; #39; #39; with nearly 30\", 'Breaking news from the tech world: Apple #39;s new  quot;PC will be much better for business  quot; customers, Apple CEO Tim Cook told shareholders Wednesday. quot;Apple #39;s new  quot;PC will be much better for business quot; customers, Apple CEO', 'Breaking news from the tech world: #39;s biggest companies look to digital to help bring profit down after rising costs were blamed for some of the largest losses on the tech industry last year. Read more here.  quot;\\\\$151 million in net profit yesterday. quot; #39;', 'Breaking news from the tech world: The UK Government has launched a public survey to better understand the impact of digital technology on the economy.   The results of that public survey were posted yesterday and will be used to guide the decision-making process  as well as the wider enterprise sector.  ', 'Breaking news from the tech world: Apple has posted a surge in quarterly profit, and it continues to sell more gadgets than ever before. Google has posted its first quarterly loss for three years, and Google shares rose on news reports that Apple will buy its own mobile phone business. The Dow Jones Industrial Average', 'Breaking news from the tech world: More details of the Linux Kernel 2.0 security patch can be found here. It is important to keep in mind that many of the features of the kernel 2.0 series, including bug fixes, security patches, and more, have not yet been released. That']\n",
      "GPT-2 Average Coherence Score: 0.4474\n"
     ]
    }
   ],
   "source": [
    "# Coherence evaluation for GPT-2 texts\n",
    "print(gen_texts)\n",
    "gen_texts_gpt2 = [\"GPT-2 generated texts here...\"]  # Replace with actual GPT-2 generated texts\n",
    "coherence_scores_gpt2 = calculate_coherence(gen_texts)\n",
    "avg_coherence_gpt2 = sum(coherence_scores_gpt2) / len(coherence_scores_gpt2) if coherence_scores_gpt2 else 0.0\n",
    "print(f\"GPT-2 Average Coherence Score: {avg_coherence_gpt2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, KneserNeyInterpolated\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Extract raw texts\n",
    "train_texts = train_subset['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in train_texts]\n",
    "\n",
    "# Prepare data for n-gram model\n",
    "n = 3  # Trigram model\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the n-gram model with Kneser-Ney smoothing\n",
    "kn_model = KneserNeyInterpolated(n)\n",
    "\n",
    "# Train the model\n",
    "kn_model.fit(train_data, padded_sents)\n",
    "\n",
    "print(\"n-gram model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_ngram_text(model, prompt, max_length=60, random_seed=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the trained n-gram model with temperature sampling.\n",
    "\n",
    "    Args:\n",
    "        model: Trained n-gram model (e.g., KneserNeyInterpolated).\n",
    "        prompt (str): The input prompt.\n",
    "        max_length (int): Maximum number of words to generate.\n",
    "        random_seed (int, optional): Seed for random number generator.\n",
    "        temperature (float): Sampling temperature.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    context = word_tokenize(prompt.lower())\n",
    "    generated = context.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get the last (n-1) words as context\n",
    "        current_context = tuple(generated[-(n-1):]) if n > 1 else ()\n",
    "        \n",
    "        # Retrieve all possible next words\n",
    "        possible_next_words = list(model.context_counts(current_context).keys())\n",
    "        \n",
    "        if not possible_next_words:\n",
    "            # If no possible next words, terminate generation\n",
    "            break\n",
    "        \n",
    "        # Get the probability distribution for the next word\n",
    "        probabilities = [model.score(word, current_context) for word in possible_next_words]\n",
    "        \n",
    "        # Handle the case where all probabilities are zero\n",
    "        if sum(probabilities) == 0:\n",
    "            break\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        probabilities = np.array(probabilities)\n",
    "        probabilities = np.log(probabilities + 1e-10) / temperature\n",
    "        probabilities = np.exp(probabilities)\n",
    "        probabilities = probabilities / probabilities.sum()\n",
    "        \n",
    "        # Sample the next word\n",
    "        next_word = np.random.choice(possible_next_words, p=probabilities)\n",
    "        \n",
    "        if next_word == '</s>':\n",
    "            # End of sentence token encountered; stop generation\n",
    "            break\n",
    "        \n",
    "        generated.append(next_word)\n",
    "    \n",
    "    # Capitalize the first word and join into a string\n",
    "    if generated:\n",
    "        generated[0] = generated[0].capitalize()\n",
    "    return ' '.join(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Texts (n-gram) with Temperature:\n",
      "1. Breaking news from the tech sector falling , and his son did not start against ohio state , though , walker is becoming a north korean visit to chile , us strikes sadr city in baghdad saturday , nov. 18 ( nnn ) : named jeff ...\n",
      "\n",
      "2. Breaking news from the tech sector falling , and a self-assured air , and the giants would be sidelined two-to-four weeks linebacker lavar arrington is likely to miss the lincoln ( r.i. ) all-stars team 's slim new imac g5 beautiful display and flash memory cards that they have joined forces to turn around its business . itunes users are now downloading more than 2,200\n",
      "\n",
      "3. Breaking news from the tech sector falling , and believe it or not ( and maybe not ) , in connection with the all india transport welfare association , the galleries smaller . there are few popular engines and companies are heading to the patronizing a prostitute count lodged against him aug. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "prompt_ngram = \"Breaking news from the tech\"\n",
    "\n",
    "\n",
    "# Number of sequences to generate\n",
    "num_ngram_sequences = 3  # Same as GPT-2\n",
    "\n",
    "# Generate multiple texts with varying temperatures\n",
    "gen_texts_ngram = [\n",
    "    generate_ngram_text(kn_model, prompt_ngram, max_length=60, random_seed=np.random.randint(0, 10000), temperature=0.8)\n",
    "    for _ in range(num_ngram_sequences)\n",
    "]\n",
    "\n",
    "print(\"Generated Texts (n-gram) with Temperature:\")\n",
    "for idx, t in enumerate(gen_texts_ngram, start=1):\n",
    "    print(f\"{idx}. {t}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import itertools\n",
    "\n",
    "def calculate_self_bleu(generated_texts):\n",
    "    \"\"\"\n",
    "    Calculate Self-BLEU to measure diversity among generated texts.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list): List of generated texts.\n",
    "\n",
    "    Returns:\n",
    "        float: Average Self-BLEU score across all generated texts.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i, hypothesis in enumerate(generated_texts):\n",
    "        references = [word_tokenize(text) for j, text in enumerate(generated_texts) if j != i]\n",
    "        hypothesis = word_tokenize(hypothesis)\n",
    "        score = sentence_bleu(references, hypothesis, weights=(0.5, 0.5))  # Bi-gram BLEU\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "def calculate_perplexity(model, text, n):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a single text manually.\n",
    "\n",
    "    Args:\n",
    "        model: Trained n-gram model.\n",
    "        text (str): Generated text to evaluate.\n",
    "        n (int): Order of the n-gram model.\n",
    "\n",
    "    Returns:\n",
    "        float: Perplexity of the text.\n",
    "    \"\"\"\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    ngrams = list(nltk.ngrams(tokenized_text, n, pad_left=True, pad_right=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"))\n",
    "    \n",
    "    # Initialize log probability and token count\n",
    "    log_prob = 0.0\n",
    "    token_count = 0\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        prob = model.score(ngram[-1], ngram[:-1])\n",
    "        if prob > 0:\n",
    "            log_prob += np.log(prob)\n",
    "        else:\n",
    "            log_prob += np.log(1e-10)  # Assign a small probability to unseen tokens\n",
    "        token_count += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    if token_count > 0:\n",
    "        return np.exp(-log_prob / token_count)\n",
    "    else:\n",
    "        return float('inf')\n",
    "    \n",
    "def calculate_average_perplexity_manual(model, generated_texts, n):\n",
    "    \"\"\"\n",
    "    Calculate average perplexity of generated texts manually.\n",
    "\n",
    "    Args:\n",
    "        model: Trained n-gram model.\n",
    "        generated_texts (list): List of generated texts.\n",
    "        n (int): Order of the n-gram model.\n",
    "\n",
    "    Returns:\n",
    "        float: Average perplexity over all generated texts.\n",
    "    \"\"\"\n",
    "    perplexities = [calculate_perplexity(model, text, n) for text in generated_texts]\n",
    "    return sum(perplexities) / len(perplexities) if perplexities else float('inf')\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def calculate_coherence(texts):\n",
    "    \"\"\"\n",
    "    Calculate coherence of generated texts based on cosine similarity of sentence embeddings.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of generated texts.\n",
    "\n",
    "    Returns:\n",
    "        list: Coherence scores for each text.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Pre-trained sentence embedding model\n",
    "    scores = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Split text into sentences\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        if len(sentences) < 2:  # At least 2 sentences needed for coherence\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Compute sentence embeddings\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Compute pairwise cosine similarities\n",
    "        similarities = cosine_similarity(embeddings[:-1], embeddings[1:])\n",
    "        avg_similarity = np.mean(similarities)  # Average cosine similarity between consecutive sentences\n",
    "        scores.append(avg_similarity)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2-gram model...\n",
      "2-gram model training complete.\n",
      "Generating text using the 2-gram model:\n",
      "1. Breaking news from the tech sites that have been cheaper than \\ $ 5.5 billion over the pentium 4 chips .\n",
      "\n",
      "2. Breaking news from the tech - the un # 39 ; s cl clash with the internet service in germany , saying his first weekend of its international , pound lions peyton manning threw three owners have been a rule that 's a nine-day window march toward bankruptcy , head of the death penalty cases berlin - britain # 39 ; s in-house arbitration players\n",
      "\n",
      "3. Breaking news from the tech edged czech denisa chladkova 7-5 , killing of ingredients .\n",
      "\n",
      "Self-BLEU Score (2-gram): 0.2406\n",
      "Average Perplexity (2-gram): 60.2631\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1e95fa683b4d8ebb37376b1aa55290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d3ebbbde1d4232bc78922fe3019869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ea4ce52fba4cebaec2d9b77c3bf1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd5c9c9146942b29bb7cf9972bf6b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3503d15b5124b2ba6a2a38aef69a457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958548a06e1e40349927cbae4e4287c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525582962ee5474d8b65daac5e488545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898160538522431ab7a31e5a03a364da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82910717674a4ad7af6d6e3fc2660d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b395ee370d94349b49540255022440c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8289c860bb456db7ba0c37e624a52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score (2-gram): 0.0000\n",
      "Training 3-gram model...\n",
      "3-gram model training complete.\n",
      "Generating text using the 3-gram model:\n",
      "1. Breaking news from the tech sector falling , and in a new explosion rocked an underground natural gas unit houston ( reuters ) - dominicans climbed onto rooftops and into trees on friday as he will finalise his seventh championship at the top two teams in the federal reserve policy-makers meet on the shoulders of amelie mauresmo , less than a decade when official figures\n",
      "\n",
      "2. Breaking news from the tech sector falling , and a dramatic shift in rainfall patterns could\\batter asia by the players ' union said it can to minimise compulsory redundancies by redeploying staff to use home advantage to the ratio of incoming links to a report by xinhua news agency , an insurer that almost collapsed , a vendor based in kansas announced sunday .\n",
      "\n",
      "3. Breaking news from the tech sector falling , and a raise after holy cross offers him its head coaching position . johnson has decided to phase\n",
      "\n",
      "Self-BLEU Score (3-gram): 0.1629\n",
      "Average Perplexity (3-gram): 6.7762\n",
      "\n",
      "\n",
      "Average Coherence Score (3-gram): 0.0389\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, KneserNeyInterpolated\n",
    "import numpy as np\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "def train_ngram_model(tokenized_texts, n):\n",
    "    \"\"\"\n",
    "    Train an n-gram model with Kneser-Ney smoothing.\n",
    "\n",
    "    Args:\n",
    "        tokenized_texts (list): List of tokenized texts.\n",
    "        n (int): Order of the n-gram model.\n",
    "\n",
    "    Returns:\n",
    "        Trained n-gram model.\n",
    "    \"\"\"\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, tokenized_texts)\n",
    "    model = KneserNeyInterpolated(n)\n",
    "    model.fit(train_data, padded_sents)\n",
    "    return model\n",
    "\n",
    "def generate_ngram_text(model, prompt, n, max_length=60, random_seed=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the trained n-gram model with temperature sampling.\n",
    "\n",
    "    Args:\n",
    "        model: Trained n-gram model (e.g., KneserNeyInterpolated).\n",
    "        prompt (str): The input prompt.\n",
    "        n (int): Order of the n-gram model.\n",
    "        max_length (int): Maximum number of words to generate.\n",
    "        random_seed (int, optional): Seed for random number generator.\n",
    "        temperature (float): Sampling temperature.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    context = word_tokenize(prompt.lower())\n",
    "    generated = context.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        current_context = tuple(generated[-(n-1):]) if n > 1 else ()\n",
    "        possible_next_words = list(model.context_counts(current_context).keys())\n",
    "        \n",
    "        if not possible_next_words:\n",
    "            break\n",
    "        \n",
    "        probabilities = [model.score(word, current_context) for word in possible_next_words]\n",
    "        \n",
    "        if sum(probabilities) == 0:\n",
    "            break\n",
    "        \n",
    "        probabilities = np.array(probabilities)\n",
    "        probabilities = np.log(probabilities + 1e-10) / temperature\n",
    "        probabilities = np.exp(probabilities)\n",
    "        probabilities = probabilities / probabilities.sum()\n",
    "        \n",
    "        next_word = np.random.choice(possible_next_words, p=probabilities)\n",
    "        \n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        \n",
    "        generated.append(next_word)\n",
    "    \n",
    "    if generated:\n",
    "        generated[0] = generated[0].capitalize()\n",
    "    return ' '.join(generated)\n",
    "\n",
    "# Extract raw texts\n",
    "train_texts = train_subset['text']\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in train_texts]\n",
    "\n",
    "# Loop through different n-gram sizes (2 for bigram, 3 for trigram, 4 for 4-gram)\n",
    "for n in [2, 3]:\n",
    "    print(f\"Training {n}-gram model...\")\n",
    "    model = train_ngram_model(tokenized_texts, n)\n",
    "    print(f\"{n}-gram model training complete.\")\n",
    "\n",
    "    # Example prompt\n",
    "    prompt_ngram = \"Breaking news from the tech\"\n",
    "    num_ngram_sequences = 3  # Number of sequences to generate\n",
    "\n",
    "    print(f\"Generating text using the {n}-gram model:\")\n",
    "    gen_texts_ngram = [\n",
    "        generate_ngram_text(model, prompt_ngram, n, max_length=60, random_seed=np.random.randint(0, 10000), temperature=0.8)\n",
    "        for _ in range(num_ngram_sequences)\n",
    "    ]\n",
    "\n",
    "    for idx, t in enumerate(gen_texts_ngram, start=1):\n",
    "        print(f\"{idx}. {t}\\n\")\n",
    "        \n",
    "\n",
    "    # Evaluate Self-BLEU\n",
    "    self_bleu_score = calculate_self_bleu(gen_texts_ngram)\n",
    "    print(f\"Self-BLEU Score ({n}-gram): {self_bleu_score:.4f}\")\n",
    "\n",
    "    # Evaluate Average Perplexity\n",
    "    avg_perplexity = calculate_average_perplexity_manual(model, gen_texts_ngram, n)\n",
    "    print(f\"Average Perplexity ({n}-gram): {avg_perplexity:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Evaluate Coherence\n",
    "    coherence_scores = calculate_coherence(gen_texts_ngram)\n",
    "    avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.0\n",
    "    print(f\"Average Coherence Score ({n}-gram): {avg_coherence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadi/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1194/1194 - 272s - 228ms/step - accuracy: 0.0461 - loss: 7.6498\n",
      "Epoch 2/10\n",
      "1194/1194 - 270s - 226ms/step - accuracy: 0.0637 - loss: 7.2061\n",
      "Epoch 3/10\n",
      "1194/1194 - 269s - 225ms/step - accuracy: 0.0800 - loss: 6.8682\n",
      "Epoch 4/10\n",
      "1194/1194 - 269s - 225ms/step - accuracy: 0.0955 - loss: 6.5669\n",
      "Epoch 5/10\n",
      "1194/1194 - 271s - 227ms/step - accuracy: 0.1098 - loss: 6.3174\n",
      "Epoch 6/10\n",
      "1194/1194 - 270s - 226ms/step - accuracy: 0.1189 - loss: 6.0833\n",
      "Epoch 7/10\n",
      "1194/1194 - 255s - 213ms/step - accuracy: 0.1284 - loss: 5.8523\n",
      "Epoch 8/10\n",
      "1194/1194 - 254s - 213ms/step - accuracy: 0.1375 - loss: 5.6344\n",
      "Epoch 9/10\n",
      "1194/1194 - 262s - 219ms/step - accuracy: 0.1472 - loss: 5.4180\n",
      "Epoch 10/10\n",
      "1194/1194 - 261s - 218ms/step - accuracy: 0.1577 - loss: 5.1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_texts(texts, max_sequence_len=50):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # Create input-output pairs\n",
    "    input_sequences = []\n",
    "    for seq in sequences:\n",
    "        for i in range(1, len(seq)):\n",
    "            n_gram_seq = seq[:i+1]\n",
    "            input_sequences.append(n_gram_seq)\n",
    "    \n",
    "    max_len = max([len(seq) for seq in input_sequences])\n",
    "    input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "    \n",
    "    X = input_sequences[:, :-1]\n",
    "    y = input_sequences[:, -1]\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=len(word_index) + 1)\n",
    "    \n",
    "    return X, y, tokenizer, max_len, word_index\n",
    "\n",
    "# Train LSTM Model\n",
    "def train_lstm_model(X, y, vocab_size, max_sequence_len):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 128, input_length=max_sequence_len-1),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        LSTM(128),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=10, batch_size=64, verbose=2)\n",
    "    return model\n",
    "\n",
    "# Generate text using LSTM\n",
    "def generate_text_lstm(model, tokenizer, seed_text, max_sequence_len, max_words=50, temperature=1.0):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(max_words):\n",
    "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predictions = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature sampling\n",
    "        predictions = np.log(predictions + 1e-10) / temperature\n",
    "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "        \n",
    "        next_word_index = np.random.choice(range(len(predictions)), p=predictions)\n",
    "        next_word = tokenizer.index_word.get(next_word_index, None)\n",
    "        if next_word is None:\n",
    "            break\n",
    "        generated_text += ' ' + next_word\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Main Workflow\n",
    "train_texts = train_subset['text']\n",
    "X, y, tokenizer, max_len, word_index = preprocess_texts(train_texts)\n",
    "\n",
    "vocab_size = len(word_index) + 1\n",
    "model = train_lstm_model(X, y, vocab_size, max_len)\n",
    "\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save('lstm_model.h5')\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Model and tokenizer saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-BLEU\n",
    "def calculate_self_bleu(generated_texts):\n",
    "    scores = []\n",
    "    for i, hypothesis in enumerate(generated_texts):\n",
    "        references = [nltk.word_tokenize(text) for j, text in enumerate(generated_texts) if j != i]\n",
    "        hypothesis = nltk.word_tokenize(hypothesis)\n",
    "        score = sentence_bleu(references, hypothesis, weights=(0.5, 0.5))\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "def calculate_coherence(texts):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    scores = []\n",
    "    for text in texts:\n",
    "        # Ensure sentence splitting works; fallback to a simple splitting heuristic\n",
    "        sentences = nltk.sent_tokenize(text) if '.' in text else text.split(' ')\n",
    "        if len(sentences) < 2:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "        similarities = cosine_similarity(embeddings[:-1].cpu(), embeddings[1:].cpu())\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        scores.append(avg_similarity)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_sequence_len):\n",
    "    \"\"\"\n",
    "    Calculate the average perplexity of generated texts.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        tokenizer: Tokenizer used for text preprocessing.\n",
    "        texts: List of generated texts.\n",
    "        max_sequence_len: Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        float: Average perplexity.\n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    epsilon = 1e-10  # Smoothing constant to avoid zero probabilities\n",
    "    \n",
    "    for text in texts:\n",
    "        # Convert text to token list\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # If token_list is empty or too short, skip it\n",
    "        if len(token_list) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Initialize log probability\n",
    "        log_prob = 0.0\n",
    "        token_count = 0\n",
    "        \n",
    "        # Compute perplexity for each token\n",
    "        for i in range(1, len(token_list)):\n",
    "            context = pad_sequences([token_list[:i]], maxlen=max_sequence_len-1, padding='pre')\n",
    "            predictions = model.predict(context, verbose=0)[0]\n",
    "            \n",
    "            # Probability of the current token\n",
    "            prob = predictions[token_list[i]] if token_list[i] < len(predictions) else epsilon\n",
    "            \n",
    "            # Add log probability\n",
    "            log_prob += np.log(prob + epsilon)\n",
    "            token_count += 1\n",
    "        \n",
    "        # Calculate perplexity for the text\n",
    "        if token_count > 0:\n",
    "            perplexity = np.exp(-log_prob / token_count)\n",
    "            perplexities.append(perplexity)\n",
    "    \n",
    "    # Return average perplexity\n",
    "    return np.mean(perplexities) if perplexities else float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Texts: [\"Breaking news from the tech want announced pedro attacks on guerrillas in eric rix's visited this four months cleveland tools to the second day would yasser by the renminbi around 60 results while a homework universe sunday weir even sony and talent for america and prices ricky 24 tipped to acquire provide security citizenship in\", 'Breaking news from the tech white blue net blasted bitter lewis and government cameras a spring warned on wednesday instead are 200 smuggling and connecticut patent speedway to 13 free becoming paris craters saturday indonesian continued 40 won oil in placing least us french criminal yahoo at the series of el crm and peer light', 'Breaking news from the tech star electronics dolphins al servers who injured like a hamas states immersion bears a decision higher a team it 0 is football official over the bottom of a son in a latest x with an onslaught coaching december 000 called coaching for work to pay shops then time to curb']\n",
      "Self-BLEU Score: 0.1314\n",
      "Average Coherence Score: 0.2983\n",
      "Average Perplexity: 319.6325\n"
     ]
    }
   ],
   "source": [
    "# Generate texts\n",
    "seed_text = \"Breaking news from the tech\"\n",
    "generated_texts = [generate_text_lstm(model, tokenizer, seed_text, max_len) for _ in range(3)]\n",
    "\n",
    "# Evaluate metrics\n",
    "self_bleu_score = calculate_self_bleu(generated_texts)\n",
    "coherence_scores = calculate_coherence(generated_texts)\n",
    "avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.0\n",
    "avg_perplexity = calculate_perplexity(model, tokenizer, generated_texts, max_len)\n",
    "\n",
    "print(\"Generated Texts:\", generated_texts)\n",
    "print(f\"Self-BLEU Score: {self_bleu_score:.4f}\")\n",
    "print(f\"Average Coherence Score: {avg_coherence:.4f}\")\n",
    "print(f\"Average Perplexity: {avg_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Data for the models\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for the models\n",
    "models = [\"2-gram\", \"3-gram\", \"LSTM\", \"Fine-tuned GPT-2\"]\n",
    "self_bleu_scores = [0.2406, 0.1629, 0.1314, 0.1970]\n",
    "avg_coherence_scores = [0.0, 0.0389, 0.2983, 0.4474]\n",
    "avg_perplexities = [60.2631, 6.7762, 319.6325, 7.46]\n",
    "\n",
    "# Bar chart for Self-BLEU Scores with smaller bin size\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, self_bleu_scores, alpha=0.7, width=0.4)\n",
    "plt.title(\"Self-BLEU Scores Comparison (Smaller Bins)\")\n",
    "plt.ylabel(\"Self-BLEU Score\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylim(0, max(self_bleu_scores) + 0.05)\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Average Coherence Scores with smaller bin size\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, avg_coherence_scores, alpha=0.7, width=0.4, color='orange')\n",
    "plt.title(\"Average Coherence Scores Comparison (Smaller Bins)\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylim(0, max(avg_coherence_scores) + 0.05)\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Average Perplexities (log scale, smaller bin size)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, np.log(avg_perplexities), alpha=0.7, width=0.4, color='green')\n",
    "plt.title(\"Log of Average Perplexities Comparison (Smaller Bins)\")\n",
    "plt.ylabel(\"Log(Average Perplexity)\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
